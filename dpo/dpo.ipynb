{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b82f8f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T12:06:53.848551Z",
     "start_time": "2025-10-10T12:06:52.402190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/manasi/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: numpy in /Users/manasi/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.22.2-py3-none-macosx_12_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tqdm in /Users/manasi/anaconda3/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in /Users/manasi/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/manasi/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/manasi/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/manasi/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: click>=8.0.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /Users/manasi/anaconda3/lib/python3.12/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from wandb) (6.32.1)\n",
      "Requirement already satisfied: pydantic<3 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from wandb) (2.11.9)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.42.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: anyio in /Users/manasi/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/manasi/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/manasi/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.2)\n",
      "Requirement already satisfied: idna in /Users/manasi/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Downloading torch-2.9.0-cp312-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.2.0-py3-none-any.whl (506 kB)\n",
      "Downloading tiktoken-0.12.0-cp312-cp312-macosx_11_0_arm64.whl (994 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.22.2-py3-none-macosx_12_0_arm64.whl (18.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Downloading sentry_sdk-2.42.0-py2.py3-none-any.whl (379 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, sympy, sentry-sdk, safetensors, pyarrow, multiprocess, hf-xet, torch, tiktoken, huggingface-hub, wandb, tokenizers, transformers, datasets\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 16.1.0\n",
      "    Uninstalling pyarrow-16.1.0:\n",
      "      Successfully uninstalled pyarrow-16.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.32.1 which is incompatible.\n",
      "streamlit 1.37.1 requires rich<14,>=10.14.0, but you have rich 14.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-4.2.0 hf-xet-1.1.10 huggingface-hub-0.35.3 multiprocess-0.70.16 pyarrow-21.0.0 safetensors-0.6.2 sentry-sdk-2.42.0 sympy-1.14.0 tiktoken-0.12.0 tokenizers-0.22.1 torch-2.9.0 transformers-4.57.1 wandb-0.22.2 xxhash-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc39729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf<6\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting rich<14\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from rich<14) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from rich<14) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/manasi/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14) (0.1.0)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Installing collected packages: protobuf, rich\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.32.1\n",
      "    Uninstalling protobuf-6.32.1:\n",
      "      Successfully uninstalled protobuf-6.32.1\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 14.1.0\n",
      "    Uninstalling rich-14.1.0:\n",
      "      Successfully uninstalled rich-14.1.0\n",
      "Successfully installed protobuf-5.29.5 rich-13.9.4\n"
     ]
    }
   ],
   "source": [
    "# Bring protobuf and rich back into Streamlit’s requested ranges\n",
    "!pip install \"protobuf<6\" \"rich<14\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "876dd92d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T12:07:57.443302Z",
     "start_time": "2025-10-10T12:07:55.144014Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# Configuration\n",
    "beta = 0.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 1e-4\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "max_length =64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "# tokenizer\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03655c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T12:08:14.793252Z",
     "start_time": "2025-10-10T12:08:14.786999Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss \n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceae772a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T12:10:52.611079Z",
     "start_time": "2025-10-10T12:10:52.355415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "\n",
    "def build_dataset(n=100000, out_path=\"./data/pos_neg_pairs.json\"):\n",
    "    pairs = []\n",
    "    for _ in range(n):\n",
    "        a, b = random.randint(1,100), random.randint(1,100)\n",
    "        op = random.choice([\"+\", \"-\", \"*\"])\n",
    "        if op == \"+\":\n",
    "            ans = a+b; reason = f\"{a}+{b} equals {ans}\"\n",
    "        elif op == \"-\":\n",
    "            ans = a-b; reason = f\"{a}-{b} equals {ans}\"\n",
    "        else:\n",
    "            ans = a*b; reason = f\"{a}*{b} equals {ans}\"\n",
    "        q = f\"{a}{op}{b}, x=?\"\n",
    "        pos = f\"{q} The answer is {ans} because {reason}.\"\n",
    "        neg = f\"{q} Sorry, I do not know!\"\n",
    "        pairs.append({\"negative\": neg, \"positive\": pos})\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(pairs, f, indent=2)\n",
    "    print(f\"Saved {len(pairs)} pairs to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df0c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "#optimizer (AdamW)\n",
    "optimizer = AdamW(gpt.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "#scheduler\n",
    "num_training_steps = 1000  # for example\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca312a9bc03ea15a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f72910a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = \"./dpo.pt\"   # <-- use this everywhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed073764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 preference pairs (raw).\n"
     ]
    }
   ],
   "source": [
    "import json, itertools, torch\n",
    "\n",
    "# ---------- 1) Load the raw JSON ----------\n",
    "with open(\"./pos_neg_pairs.json\", \"r\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "# Some files wrap the list, e.g. {\"pairs\":[...]} / {\"data\":[...]}\n",
    "if isinstance(raw, dict):\n",
    "    for k in (\"pairs\",\"data\",\"examples\",\"items\"):\n",
    "        if k in raw and isinstance(raw[k], list):\n",
    "            pairs = raw[k]\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(\"Couldn't find a list of pairs inside the JSON dict.\")\n",
    "elif isinstance(raw, list):\n",
    "    pairs = raw\n",
    "else:\n",
    "    raise ValueError(\"JSON must be a list or a dict containing a list.\")\n",
    "\n",
    "print(f\"Loaded {len(pairs)} preference pairs (raw).\")\n",
    "\n",
    "# ---------- 2) Figure out the schema ----------\n",
    "def extract_pair(p):\n",
    "    \"\"\"\n",
    "    Return (neg_text, pos_text) as strings from one item p, trying several common schemas.\n",
    "    If a 'prompt' exists, it will be prefixed to both completions.\n",
    "    \"\"\"\n",
    "    # Helper to optionally prepend prompt/context\n",
    "    def join_prompt(prompt, ans):\n",
    "        if prompt:\n",
    "            # keep a simple, consistent format for math/QA\n",
    "            return f\"{prompt.strip()}\\n{ans.strip()}\"\n",
    "        return ans.strip()\n",
    "\n",
    "    # Case A: dict-like with various key names\n",
    "    if isinstance(p, dict):\n",
    "        d = {k.lower(): v for k, v in p.items()}\n",
    "\n",
    "        # Candidate key pairs in order of likelihood\n",
    "        key_pairs = [\n",
    "            (\"neg\",\"pos\"),\n",
    "            (\"negative\",\"positive\"),\n",
    "            (\"rejected\",\"chosen\"),\n",
    "            (\"dispreferred\",\"preferred\"),\n",
    "            (\"bad\",\"good\"),\n",
    "            (\"worse\",\"better\"),\n",
    "            (\"lose\",\"win\"),\n",
    "            (\"neg_resp\",\"pos_resp\"),\n",
    "            (\"neg_text\",\"pos_text\"),\n",
    "            (\"completion_b\",\"completion_a\"),  # some datasets name A=preferred\n",
    "        ]\n",
    "\n",
    "        prompt = d.get(\"prompt\") or d.get(\"question\") or d.get(\"input\") or d.get(\"context\") or \"\"\n",
    "\n",
    "        for neg_k, pos_k in key_pairs:\n",
    "            if neg_k in d and pos_k in d:\n",
    "                neg_text = join_prompt(prompt, str(d[neg_k]))\n",
    "                pos_text = join_prompt(prompt, str(d[pos_k]))\n",
    "                return neg_text, pos_text\n",
    "\n",
    "        # Another common form: have \"chosen\"/\"rejected\"\n",
    "        if \"chosen\" in d and \"rejected\" in d:\n",
    "            neg_text = join_prompt(prompt, str(d[\"rejected\"]))\n",
    "            pos_text = join_prompt(prompt, str(d[\"chosen\"]))\n",
    "            return neg_text, pos_text\n",
    "\n",
    "        # Another: have \"pos\"/\"neg\" but plus a separate \"answer\" keys\n",
    "        if \"pos\" in d and \"neg\" in d:\n",
    "            neg_text = join_prompt(prompt, str(d[\"neg\"]))\n",
    "            pos_text = join_prompt(prompt, str(d[\"pos\"]))\n",
    "            return neg_text, pos_text\n",
    "\n",
    "        # If there are only two text fields but weird names, try to guess:\n",
    "        # look for the two longest string fields as candidates\n",
    "        text_fields = [(k,v) for k,v in d.items() if isinstance(v, str)]\n",
    "        if len(text_fields) >= 2:\n",
    "            # heuristic: prefer keys hinting pos/neg if present\n",
    "            name = \" \".join(d.keys()).lower()\n",
    "            # fallback: pick two and assume the first is negative if its key looks like neg/bad/reject\n",
    "            # else we’ll try length heuristic\n",
    "            neg_like = [k for k,_ in text_fields if any(tag in k for tag in [\"neg\",\"bad\",\"reject\",\"worse\",\"lose\",\"dispref\"])]\n",
    "            pos_like = [k for k,_ in text_fields if any(tag in k for tag in [\"pos\",\"good\",\"chosen\",\"better\",\"win\",\"pref\"])]\n",
    "            if neg_like and pos_like:\n",
    "                neg_text = join_prompt(prompt, str(d[neg_like[0]]))\n",
    "                pos_text = join_prompt(prompt, str(d[pos_like[0]]))\n",
    "                return neg_text, pos_text\n",
    "            # length heuristic\n",
    "            text_fields_sorted = sorted(text_fields, key=lambda kv: -len(kv[1]))\n",
    "            pos_text = join_prompt(prompt, text_fields_sorted[0][1])\n",
    "            neg_text = join_prompt(prompt, text_fields_sorted[1][1])\n",
    "            return neg_text, pos_text\n",
    "\n",
    "        raise KeyError(\"Unrecognized dict schema for a pair item.\")\n",
    "\n",
    "    # Case B: list/tuple of two strings\n",
    "    if isinstance(p, (list, tuple)) and len(p) >= 2 and all(isinstance(x, str) for x in p[:2]):\n",
    "        # Heuristic: assume index 0 = positive if marker present, else index 1 = positive.\n",
    "        # To be safe for DPO, we’ll assume [pos, neg]; flip if you know your file is [neg, pos].\n",
    "        pos_text, neg_text = p[0].strip(), p[1].strip()\n",
    "        return neg_text, pos_text\n",
    "\n",
    "    raise KeyError(\"Unrecognized pair item type.\")\n",
    "\n",
    "# ---------- 3) Build encoded dataset ----------\n",
    "# Require the same tokenizer mapping (stoi/itos) you loaded earlier from meta.pkl\n",
    "def encode_text(txt):\n",
    "    return torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "173f9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "PAD_ID = 0  # you padded with 0 above; keep consistent\n",
    "\n",
    "def make_shifted_targets(x, lengths):\n",
    "    \"\"\"\n",
    "    x: (B, T) input ids (already padded with PAD_ID)\n",
    "    lengths: list/1D tensor of true lengths for each row (without right pad)\n",
    "    Returns y with shape (B, T) where:\n",
    "      y[:, :-1] = x[:, 1:], y[:, -1] = PAD_ID, and\n",
    "      positions >= (length-1) are set to -100 (ignore_index) so CE ignores them.\n",
    "    \"\"\"\n",
    "    B, T = x.shape\n",
    "    y = x.clone()\n",
    "    y[:, :-1] = x[:, 1:]\n",
    "    y[:, -1] = PAD_ID\n",
    "    y = y.long()\n",
    "    # mask everything at/after each (length-1)\n",
    "    y_mask = torch.zeros_like(y).bool()\n",
    "    for i, L in enumerate(lengths):\n",
    "        cut = max(int(L) - 1, 0)\n",
    "        if cut < T:\n",
    "            y_mask[i, cut:] = True\n",
    "    y[y_mask] = -100  # ignore_index\n",
    "    return y\n",
    "\n",
    "def sequence_mean_logprob(model, x, lengths):\n",
    "    \"\"\"\n",
    "    Compute mean log-prob per example, ignoring padding (via -100 targets).\n",
    "    Returns tensor of shape (B,) with average logprob per token for each sequence.\n",
    "    \"\"\"\n",
    "    y = make_shifted_targets(x, lengths)             # (B, T) with -100 masked\n",
    "    logits, _ = model(x, y=None)                     # logits: (B, T, V)\n",
    "    logprobs = F.log_softmax(logits, dim=-1)         # (B, T, V)\n",
    "    # gather at target ids (valid positions have y>=0; ignored are -100)\n",
    "    B, T = y.shape\n",
    "    gather_y = y.clone()\n",
    "    gather_y[gather_y < 0] = 0                       # placeholder index for ignored\n",
    "    token_lp = logprobs.gather(2, gather_y.unsqueeze(-1)).squeeze(-1)  # (B, T)\n",
    "    # zero out ignored positions\n",
    "    mask = (y != -100).float()\n",
    "    # sum per sequence and divide by number of valid tokens\n",
    "    seq_sum = (token_lp * mask).sum(dim=1)           # (B,)\n",
    "    seq_cnt = mask.sum(dim=1).clamp_min(1.0)         # avoid /0\n",
    "    return seq_sum / seq_cnt                         # (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ebeb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(gpt\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      2\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      4\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lines) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "beta = 0.1   # DPO temperature (adjustable)\n",
    "total_steps = len(lines) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0   # <-- add this line\n",
    "\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n",
    "        # ---------------------------------------------------------\n",
    "        # 1. Move batches to device\n",
    "        # ---------------------------------------------------------\n",
    "        neg_tensor = neg_tensor.to(device)\n",
    "        pos_tensor = pos_tensor.to(device)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 2. Compute log-probs (mean log-likelihoods) for both\n",
    "        # ---------------------------------------------------------\n",
    "        with torch.autocast(device_type=device.type,\n",
    "                            dtype=torch.float16 if device.type != \"cpu\" else torch.float32):\n",
    "            pos_logits, pos_loss = gpt(pos_tensor)\n",
    "            neg_logits, neg_loss = gpt(neg_tensor)\n",
    "\n",
    "            pos_logprob = -pos_loss\n",
    "            neg_logprob = -neg_loss\n",
    "\n",
    "            # -----------------------------------------------------\n",
    "            # 3. DPO objective\n",
    "            # -----------------------------------------------------\n",
    "            loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean()\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 4. Backprop + step\n",
    "        # ---------------------------------------------------------\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1   # <-- increment count\n",
    "        pbar.set_description(\n",
    "            f\"Epoch {epoch+1} | Step {step+1}/{total_steps} | Loss {loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 5. End-of-epoch reporting & checkpoint\n",
    "    # -------------------------------------------------------------\n",
    "    mean_loss = epoch_loss / num_batches if num_batches > 0 else float('nan')\n",
    "    print(f\"Epoch {epoch+1} mean loss: {mean_loss:.4f}\")\n",
    "\n",
    "    ckpt_path = f\"./dpo.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"✅ Saved checkpoint to {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09027262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pickle, torch, re\n",
    "\n",
    "# device = torch.device(\n",
    "#     \"cuda\" if torch.cuda.is_available()\n",
    "#     else \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "#     else \"cpu\"\n",
    "# )\n",
    "device = torch.device(\"cpu\")\n",
    "gpt = gpt.to(device)\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "\n",
    "PAD_ID = 0  # we pad with 0 in batching\n",
    "\n",
    "import torch\n",
    "\n",
    "def encode(text: str) -> torch.Tensor:\n",
    "    # return 1D ids (we'll add batch dim in generate)\n",
    "    ids = [stoi.get(ch, 0) for ch in text]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt: str, max_new_tokens=64, temperature=1.0, top_k=1):\n",
    "    # 1) encode\n",
    "    x = encode(prompt)\n",
    "    # 2) ensure 2D: (1, T)\n",
    "    if x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    # 3) trim to block_size if needed\n",
    "    try:\n",
    "        block_size = model.config.block_size\n",
    "        if x.size(1) > block_size:\n",
    "            x = x[:, -block_size:]\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 4) move to device\n",
    "    x = x.to(device)\n",
    "\n",
    "    # normalize sampler args for your model's generate()\n",
    "    if top_k is not None and top_k <= 0:\n",
    "        top_k = 1\n",
    "    if temperature is None or temperature <= 0:\n",
    "        temperature = 1e-5\n",
    "\n",
    "    # 5) generate\n",
    "    out = model.generate(\n",
    "        x, max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature, top_k=top_k\n",
    "    )\n",
    "    if isinstance(out, tuple):\n",
    "        out = out[0]\n",
    "    return decode(out)\n",
    "\n",
    "\n",
    "def _to_list(ids):\n",
    "    if isinstance(ids, torch.Tensor): ids = ids.detach().cpu().tolist()\n",
    "    out = []\n",
    "    def flat(x):\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            for y in x: flat(y)\n",
    "        else:\n",
    "            out.append(int(x))\n",
    "    flat(ids)\n",
    "    return out\n",
    "\n",
    "def decode(ids) -> str:\n",
    "    ids = _to_list(ids)\n",
    "    if isinstance(itos, dict):\n",
    "        return \"\".join(itos.get(i, \"\") for i in ids)\n",
    "    L = len(itos)\n",
    "    return \"\".join(itos[i] if 0 <= i < L else \"\" for i in ids)\n",
    "\n",
    "def parse_last_number(text: str):\n",
    "    nums = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", text)\n",
    "    return float(nums[-1]) if nums else None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a50a78ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pairs: 100000\n",
      "Usable pairs: 100000  | Skipped: 0\n",
      "\n",
      "--- Sample 0 ---\n",
      "NEG: 95-62, x=? Sorry, I do not know\n",
      "\n",
      "POS: 95-62, x=? The answer is 33 because 95-62 equals 33.\n",
      "\n",
      "--- Sample 1 ---\n",
      "NEG: 41*42, x=? Sorry, I do not know\n",
      "\n",
      "POS: 41*42, x=? The answer is 1722 because 41*42 equals 1722.\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "with open(\"./pos_neg_pairs.json\", \"r\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "pairs_list = raw[\"pairs\"] if isinstance(raw, dict) and isinstance(raw.get(\"pairs\"), list) else raw\n",
    "print(\"Raw pairs:\", len(pairs_list))\n",
    "\n",
    "def extract_pair(p):\n",
    "    # Returns (NEG_text, POS_text). Try common schemas; adjust if needed.\n",
    "    if isinstance(p, dict):\n",
    "        d = {k.lower(): v for k,v in p.items()}\n",
    "        prompt = d.get(\"prompt\") or d.get(\"question\") or d.get(\"input\") or \"\"\n",
    "        def join(pr, ans): return (pr.strip()+\"\\n\"+str(ans).strip()) if pr else str(ans).strip()\n",
    "\n",
    "        candidates = [\n",
    "            (\"neg\",\"pos\"),\n",
    "            (\"negative\",\"positive\"),\n",
    "            (\"rejected\",\"chosen\"),\n",
    "            (\"dispreferred\",\"preferred\"),\n",
    "            (\"bad\",\"good\"),\n",
    "            (\"completion_b\",\"completion_a\"),\n",
    "        ]\n",
    "        for nk, pk in candidates:\n",
    "            if nk in d and pk in d:\n",
    "                return join(prompt, d[nk]), join(prompt, d[pk])\n",
    "        if \"chosen\" in d and \"rejected\" in d:\n",
    "            return join(prompt, d[\"rejected\"]), join(prompt, d[\"chosen\"])\n",
    "    if isinstance(p, (list, tuple)) and len(p) >= 2 and all(isinstance(x, str) for x in p[:2]):\n",
    "        # assume [POS, NEG] is common; flip to (NEG, POS)\n",
    "        return p[1], p[0]\n",
    "    raise KeyError(\"Unrecognized pair format\")\n",
    "\n",
    "# Build encoded lines (NEG_ids, POS_ids)\n",
    "lines = []\n",
    "skipped = 0\n",
    "for i, p in enumerate(pairs_list):\n",
    "    try:\n",
    "        neg_txt, pos_txt = extract_pair(p)\n",
    "        lines.append((encode(neg_txt), encode(pos_txt)))\n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "print(f\"Usable pairs: {len(lines)}  | Skipped: {skipped}\")\n",
    "\n",
    "# >>> VERY IMPORTANT: inspect a few samples <<<\n",
    "for i in range(2):\n",
    "    n, p = lines[i]\n",
    "    print(\"\\n--- Sample\", i, \"---\")\n",
    "    print(\"NEG:\", decode(n[:120]))\n",
    "    print(\"POS:\", decode(p[:120]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45c36e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        negs = [n for n, _ in batch]\n",
    "        poss = [p for _, p in batch]\n",
    "        neg_len = torch.tensor([len(t) for t in negs], dtype=torch.long)\n",
    "        pos_len = torch.tensor([len(t) for t in poss], dtype=torch.long)\n",
    "        neg_pad = rnn.pad_sequence(negs, batch_first=True, padding_value=PAD_ID)\n",
    "        pos_pad = rnn.pad_sequence(poss, batch_first=True, padding_value=PAD_ID)\n",
    "        yield (neg_pad, neg_len), (pos_pad, pos_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1567807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "PAD_ID = 0  # keep consistent with your padding\n",
    "\n",
    "# def make_targets_shifted(x, lengths):\n",
    "#     \"\"\"\n",
    "#     Shift inputs to create next-token targets and mask padding with -100.\n",
    "#     x: (B, T) token ids\n",
    "#     lengths: (B,) true lengths before right padding\n",
    "#     return: y (B, T) with -100 at ignored positions\n",
    "#     \"\"\"\n",
    "#     y = x.clone()\n",
    "#     y[:, :-1] = x[:, 1:]\n",
    "#     y[:, -1] = PAD_ID\n",
    "#     B, T = x.shape\n",
    "#     mask = torch.zeros_like(y, dtype=torch.bool)\n",
    "#     for i, L in enumerate(lengths.tolist()):\n",
    "#         cut = max(int(L) - 1, 0)\n",
    "#         if cut < T:\n",
    "#             mask[i, cut:] = True\n",
    "#     y[mask] = -100  # ignore_index for CrossEntropy\n",
    "#     return y\n",
    "\n",
    "# def mean_logprob_per_seq(model, x, lengths):\n",
    "#     \"\"\"\n",
    "#     One forward pass over the whole sequence.\n",
    "#     Works with NanoGPT-style forward that returns (logits, loss) when given targets POSITIONALLY.\n",
    "#     \"\"\"\n",
    "#     # 1) build masked targets\n",
    "#     y = make_targets_shifted(x, lengths)              # (B, T), -100 where ignored\n",
    "\n",
    "#     # 2) forward: IMPORTANT — pass targets POSITIONALLY, not as keyword\n",
    "#     # Expected: logits shape (B, T, V)\n",
    "#     out = model(x, y)                                 # <-- key change\n",
    "#     logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "#     if logits.dim() != 3:\n",
    "#         raise RuntimeError(f\"Expected logits of shape (B, T, V), got {tuple(logits.shape)}\")\n",
    "\n",
    "#     # 3) compute token log-probs and average per sequence (ignore -100)\n",
    "#     logp = F.log_softmax(logits, dim=-1)              # (B, T, V)\n",
    "\n",
    "#     y_gather = y.clone()\n",
    "#     y_gather[y_gather < 0] = 0                        # safe index for ignored\n",
    "#     token_lp = logp.gather(2, y_gather.unsqueeze(-1)).squeeze(-1)  # (B, T)\n",
    "\n",
    "#     valid = (y != -100).float()\n",
    "#     seq_sum = (token_lp * valid).sum(dim=1)           # (B,)\n",
    "#     seq_cnt = valid.sum(dim=1).clamp_min(1.0)\n",
    "#     return seq_sum / seq_cnt                          # (B,)\n",
    "\n",
    "\n",
    "def mean_logprob_per_seq(model, x, lengths):\n",
    "    \"\"\"\n",
    "    Works when model only returns last-step logits.\n",
    "    Average per-token log-prob via teacher forcing:\n",
    "      for t in [0..T-2], run model on x[:, :t+1] and score next token x[:, t+1].\n",
    "    Ignores padding using 'lengths'.\n",
    "    Returns: (B,) of mean log-prob per valid token.\n",
    "    \"\"\"\n",
    "    B, T = x.shape\n",
    "    device = x.device\n",
    "    total_logp = torch.zeros(B, device=device)\n",
    "    total_cnt  = torch.zeros(B, device=device)\n",
    "\n",
    "    # DO NOT use autocast here on MPS (saves memory)\n",
    "    for t in range(T - 1):\n",
    "        # rows that still have a next token at t+1\n",
    "        valid_mask = (lengths > (t + 1))  # (B,)\n",
    "        if not bool(valid_mask.any()):\n",
    "            break\n",
    "\n",
    "        prefix  = x[:, :t+1]          # (B, t+1)\n",
    "        next_id = x[:, t+1]           # (B,)\n",
    "\n",
    "        out = model(prefix)           # logits for last step only\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "        if logits.dim() == 3:         # (B,1,V) -> (B,V)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "        logp = F.log_softmax(logits, dim=-1)            # (B,V)\n",
    "        lp_next = logp.gather(1, next_id.unsqueeze(1)).squeeze(1)  # (B,)\n",
    "\n",
    "        total_logp = total_logp + lp_next * valid_mask.float()\n",
    "        total_cnt  = total_cnt  + valid_mask.float()\n",
    "\n",
    "    return total_logp / total_cnt.clamp_min(1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6897c218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.0000: 100%|██████████| 6250/6250 [10:35:05<00:00,  6.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 mean loss: 0.0064\n",
      "✅ Saved to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# small batch if you still see OOM; try 16 first\n",
    "batch_size = min(batch_size, 16)  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss, num_batches = 0.0, 0\n",
    "    pbar = tqdm(get_batches(lines, batch_size), total=(len(lines)+batch_size-1)//batch_size)\n",
    "    gpt.train()\n",
    "\n",
    "    for (neg_pad, neg_len), (pos_pad, pos_len) in pbar:\n",
    "        neg_pad = neg_pad.to(device);  neg_len = neg_len.to(device)\n",
    "        pos_pad = pos_pad.to(device);  pos_len = pos_len.to(device)\n",
    "\n",
    "        # IMPORTANT: no autocast on MPS to reduce memory pressure\n",
    "        neg_lp = mean_logprob_per_seq(gpt, neg_pad, neg_len)   # (B,)\n",
    "        pos_lp = mean_logprob_per_seq(gpt, pos_pad, pos_len)   # (B,)\n",
    "\n",
    "        loss = -F.logsigmoid((pos_lp - neg_lp) / beta).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item(); num_batches += 1\n",
    "        pbar.set_description(f\"Epoch {epoch+1} | Loss {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} mean loss: {epoch_loss / max(num_batches,1):.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": checkpoint['model_args'],\n",
    "    }, CKPT_PATH)\n",
    "    print(f\"✅ Saved to {CKPT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "202e5779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:24<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.06 GiB, other allocations: 14.69 MiB, max allowed: 9.07 GiB). Tried to allocate 5.95 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype,\n\u001b[1;32m     20\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[1;32m     21\u001b[0m         neg_lp \u001b[38;5;241m=\u001b[39m mean_logprob_per_seq(gpt, neg_pad, neg_len)   \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m         pos_lp \u001b[38;5;241m=\u001b[39m mean_logprob_per_seq(gpt, pos_pad, pos_len)   \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         loss   \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mF\u001b[38;5;241m.\u001b[39mlogsigmoid((pos_lp \u001b[38;5;241m-\u001b[39m neg_lp) \u001b[38;5;241m/\u001b[39m beta)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[36], line 52\u001b[0m, in \u001b[0;36mmean_logprob_per_seq\u001b[0;34m(model, x, lengths)\u001b[0m\n\u001b[1;32m     49\u001b[0m next_tok \u001b[38;5;241m=\u001b[39m x[:, t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]                       \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# forward: many tiny GPTs return logits for last step only\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m out \u001b[38;5;241m=\u001b[39m model(prefix)\n\u001b[1;32m     53\u001b[0m logits \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m out   \u001b[38;5;66;03m# (B, V) or (B, 1, V)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:     \u001b[38;5;66;03m# (B, 1, V) -> (B, V)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Downloads/NanoGPT_Math-optimiser/model.py:156\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, return_hidden_states, full_seq)\u001b[0m\n\u001b[1;32m    154\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdrop(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 156\u001b[0m     x \u001b[38;5;241m=\u001b[39m block(x)\n\u001b[1;32m    157\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Downloads/NanoGPT_Math-optimiser/model.py:90\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     89\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[0;32m---> 90\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Downloads/NanoGPT_Math-optimiser/model.py:74\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     73\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(x)\n\u001b[0;32m---> 74\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgelu(x)\n\u001b[1;32m     75\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(x)\n\u001b[1;32m     76\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/activation.py:816\u001b[0m, in \u001b[0;36mGELU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    813\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mgelu(\u001b[38;5;28minput\u001b[39m, approximate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximate)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 9.06 GiB, other allocations: 14.69 MiB, max allowed: 9.07 GiB). Tried to allocate 5.95 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "beta = 0.1\n",
    "epochs = 1           # start with 1 to sanity-check; increase later\n",
    "batch_size = 64      # whatever you used earlier\n",
    "lr = 3e-4            # example; keep your original\n",
    "\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=lr)\n",
    "\n",
    "total_steps = (len(lines) + batch_size - 1) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    gpt.train()\n",
    "    running, batches = 0.0, 0\n",
    "    pbar = tqdm(get_batches(lines, batch_size), total=total_steps)\n",
    "    for (neg_pad, neg_len), (pos_pad, pos_len) in pbar:\n",
    "        neg_pad = neg_pad.to(device);  neg_len = neg_len.to(device)\n",
    "        pos_pad = pos_pad.to(device);  pos_len = pos_len.to(device)\n",
    "\n",
    "        with torch.autocast(device_type=device.type,\n",
    "                    dtype=torch.float16 if device.type != \"cpu\" else torch.float32):\n",
    "                neg_lp = mean_logprob_per_seq(gpt, neg_pad, neg_len)   # (B,)\n",
    "                pos_lp = mean_logprob_per_seq(gpt, pos_pad, pos_len)   # (B,)\n",
    "                loss   = -F.logsigmoid((pos_lp - neg_lp) / beta).mean()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item(); batches += 1\n",
    "        pbar.set_description(f\"Epoch {epoch+1} | Loss {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} mean loss: {running / max(batches,1):.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": checkpoint['model_args'],\n",
    "    }, CKPT_PATH)\n",
    "    print(f\"✅ Saved to {CKPT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5df9666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def first_int_after_answer(text: str):\n",
    "    # Take only the substring after \"Answer\"\n",
    "    m = re.search(r'answer.*?:\\s*(.*)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "    tail = m.group(1) if m else text\n",
    "    m2 = re.search(r'[-+]?\\d+', tail)  # first integer sequence\n",
    "    return int(m2.group()) if m2 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1283c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Build the allowed charset -> token ids once\n",
    "ALLOWED_CHARS = \"0123456789-\"\n",
    "ALLOWED_IDS = [stoi.get(ch, None) for ch in ALLOWED_CHARS]\n",
    "ALLOWED_IDS = [i for i in ALLOWED_IDS if i is not None]\n",
    "assert len(ALLOWED_IDS) > 0, \"Tokenizer has no digit tokens?!\"\n",
    "\n",
    "def generate_number_only(model, prompt: str, max_digits: int = 6):\n",
    "    \"\"\"\n",
    "    Greedy decode but only allow digits (and optional '-').\n",
    "    Stops when it produced at least 1 digit and then hits a non-digit next, or max_digits reached.\n",
    "    \"\"\"\n",
    "    # 1) encode and batchify\n",
    "    x = torch.tensor([stoi.get(ch, 0) for ch in prompt], dtype=torch.long)\n",
    "    if x.dim() == 1: x = x.unsqueeze(0)                # (1, T)\n",
    "    # 2) trim to block_size if necessary\n",
    "    try:\n",
    "        block = model.config.block_size\n",
    "        if x.size(1) > block:\n",
    "            x = x[:, -block:]\n",
    "    except Exception:\n",
    "        pass\n",
    "    x = x.to(device)\n",
    "\n",
    "    produced = []\n",
    "    for t in range(max_digits):\n",
    "        # forward; many tiny GPTs return last-step logits only\n",
    "        out = model(x)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out   # (1, V) or (1,1,V)\n",
    "        if logits.dim() == 3:\n",
    "            logits = logits[:, -1, :]  # (1, V)\n",
    "\n",
    "        # mask everything except allowed ids\n",
    "        mask = torch.full_like(logits, float(\"-inf\"))\n",
    "        mask[:, ALLOWED_IDS] = 0.0\n",
    "        masked = logits + mask\n",
    "\n",
    "        # greedy pick\n",
    "        next_id = masked.argmax(dim=-1)  # (1,)\n",
    "        nid = next_id.item()\n",
    "\n",
    "        # append\n",
    "        produced.append(nid)\n",
    "\n",
    "        # append to input (incremental generation)\n",
    "        x = torch.cat([x, next_id.view(1, 1)], dim=1)\n",
    "\n",
    "        # simple stopping: if we already have ≥1 digit and model tries to repeat '-' as 2nd char, ignore;\n",
    "        # we’ll just rely on max_digits to stop.\n",
    "        # (You can also stop if the next best non-digit is far away; not needed here.)\n",
    "\n",
    "    # decode just the produced part\n",
    "    txt = \"\".join(itos[i] if isinstance(itos, list) else itos.get(i, \"\") for i in produced)\n",
    "    # sanitize: keep leading '-' then digits\n",
    "    import re\n",
    "    m = re.match(r\"^-?\\d+\", txt)\n",
    "    return m.group(0) if m else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "10fa2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "FEWSHOT = (\n",
    "    \"3+4=? The answer is 7 because 3+4 equals 7.\\n\"\n",
    "    \"12*3=? The answer is 36 because 12*3 equals 36.\\n\"\n",
    ")\n",
    "\n",
    "def format_prompt(q: str) -> str:\n",
    "    # match POS style exactly; we end with a trailing space\n",
    "    return FEWSHOT + f\"{q} The answer is \"\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen_and_parse(model, q, max_new_tokens=16, temperature=0.6, top_k=40):\n",
    "    prompt = format_prompt(q)\n",
    "    x = torch.tensor([stoi.get(ch,0) for ch in prompt], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    y = model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "    if isinstance(y, tuple): y = y[0]\n",
    "    txt = decode(y)\n",
    "\n",
    "    # *** Parse ONLY the completion part ***\n",
    "    comp = txt[len(prompt):]  # everything the model added after our prompt\n",
    "\n",
    "    # optional: cut off at common stop tokens to avoid trailing junk\n",
    "    for stop in [\" because\", \"\\n\", \".\", \" Answer\", \"The answer is\"]:\n",
    "        idx = comp.find(stop)\n",
    "        if idx > 0:\n",
    "            comp = comp[:idx]\n",
    "            break\n",
    "\n",
    "    # now extract the first integer from the completion\n",
    "    m = re.search(r\"[-+]?\\d+\", comp)\n",
    "    pred = int(m.group()) if m else None\n",
    "    return prompt, comp, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7923eca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION (few-shot; parse completion only) ===\n",
      "Q: 17+19=?\n",
      "Completion: 4142462704982724\n",
      "Parsed: 4142462704982724 | Target: 36 | ✗\n",
      "----------------------------------------------------------------------\n",
      "Q: 3*17=?\n",
      "Completion: 2704228e7294=8+1\n",
      "Parsed: 2704228 | Target: 51 | ✗\n",
      "----------------------------------------------------------------------\n",
      "Q: 72/4=?\n",
      "Completion: 346444464478+664\n",
      "Parsed: 346444464478 | Target: 18 | ✗\n",
      "----------------------------------------------------------------------\n",
      "Q: 72-x=34, x=?\n",
      "Completion: 6447463544414=14\n",
      "Parsed: 6447463544414 | Target: 38 | ✗\n",
      "----------------------------------------------------------------------\n",
      "Q: x*11=44, x=?\n",
      "Completion: 37418487472-6949\n",
      "Parsed: 37418487472 | Target: 4 | ✗\n",
      "----------------------------------------------------------------------\n",
      "Accuracy: 0/5\n"
     ]
    }
   ],
   "source": [
    "# load the same ./dpo.pt as you just saved earlier (you already do this above)\n",
    "FEWSHOT = (\n",
    "    \"3+4=? The answer is 7 because 3+4 equals 7.\\n\"\n",
    "    \"12*3=? The answer is 36 because 12*3 equals 36.\\n\"\n",
    ")\n",
    "\n",
    "def format_prompt(q: str) -> str:\n",
    "    return FEWSHOT + f\"{q} The answer is \"\n",
    "\n",
    "\n",
    "\n",
    "test_set = [\n",
    "    (\"17+19=?\", 36),\n",
    "    (\"3*17=?\", 51),\n",
    "    (\"72/4=?\", 18),\n",
    "    (\"72-x=34, x=?\", 38),\n",
    "    (\"x*11=44, x=?\", 4),\n",
    "]\n",
    "\n",
    "correct = 0\n",
    "print(\"\\n=== EVALUATION (few-shot; parse completion only) ===\")\n",
    "for q, tgt in test_set:\n",
    "    prompt, comp, pred = gen_and_parse(gpt, q, max_new_tokens=16, temperature=0.6, top_k=40)\n",
    "    ok = (pred is not None) and (pred == int(tgt))\n",
    "    correct += int(ok)\n",
    "    print(\"Q:\", q)\n",
    "    print(\"Completion:\", comp.strip().replace(\"\\n\",\" ⏎ \"))\n",
    "    print(\"Parsed:\", pred, \"| Target:\", tgt, \"|\", \"✓\" if ok else \"✗\")\n",
    "    print(\"-\"*70)\n",
    "print(f\"Accuracy: {correct}/{len(test_set)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a7e6bd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Δ (pos-neg) = 2.746781349182129\n"
     ]
    }
   ],
   "source": [
    "(neg_pad, neg_len), (pos_pad, pos_len) = next(get_batches(lines[:64], 8))\n",
    "neg_pad, neg_len = neg_pad.to(device), neg_len.to(device)\n",
    "pos_pad, pos_len = pos_pad.to(device), pos_len.to(device)\n",
    "gpt.eval()\n",
    "with torch.no_grad():\n",
    "    d = (mean_logprob_per_seq(gpt, pos_pad, pos_len) - \n",
    "         mean_logprob_per_seq(gpt, neg_pad, neg_len)).mean().item()\n",
    "print(\"Avg Δ (pos-neg) =\", d)  # should be > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4b9bd",
   "metadata": {},
   "source": [
    "did not work new try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f93d8",
   "metadata": {},
   "source": [
    "Utilities: find common prefix + build completion masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d3fd38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "PAD_ID = 0  # keep consistent\n",
    "\n",
    "def common_prefix_len(a: torch.Tensor, b: torch.Tensor):\n",
    "    \"\"\"\n",
    "    a,b: 1D Long tensors\n",
    "    Returns the length of the exact shared prefix.\n",
    "    \"\"\"\n",
    "    L = min(a.numel(), b.numel())\n",
    "    i = 0\n",
    "    while i < L and int(a[i]) == int(b[i]):\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "def build_completion_targets(padded: torch.Tensor, true_lens: torch.Tensor, comp_starts: torch.Tensor):\n",
    "    \"\"\"\n",
    "    y[:, t] = next token id to predict at t; -100 outside completion.\n",
    "    \"\"\"\n",
    "    padded = padded.long()\n",
    "    true_lens = true_lens.long()\n",
    "    comp_starts = comp_starts.long()\n",
    "\n",
    "    B, T = padded.shape\n",
    "    y = padded.clone()\n",
    "    y[:, :-1] = padded[:, 1:]\n",
    "    y[:, -1] = PAD_ID\n",
    "\n",
    "    mask = torch.zeros_like(y, dtype=torch.bool)\n",
    "    for i in range(B):\n",
    "        L = int(true_lens[i])\n",
    "        start = max(int(comp_starts[i]) - 1, 0)  # shift by one because y predicts next\n",
    "        end = max(L - 1, 0)                      # last valid prediction index\n",
    "        # mask BEFORE start\n",
    "        if start > 0: mask[i, :start] = True\n",
    "        # mask AFTER end\n",
    "        if end < T:  mask[i, end:] = True\n",
    "\n",
    "    y[mask] = -100\n",
    "    return y\n",
    "\n",
    "\n",
    "    # Slow path: last-token logits → loop over time\n",
    "    B, T = padded.shape\n",
    "    device = padded.device\n",
    "    total_lp = torch.zeros(B, device=device)\n",
    "    total_cnt = torch.zeros(B, device=device)\n",
    "\n",
    "    for t in range(T):\n",
    "        y_t = y[:, t]\n",
    "        valid = (y_t >= 0)  # only positions inside the completion span\n",
    "        if not bool(valid.any()):\n",
    "            continue\n",
    "\n",
    "        prefix = padded[:, :t+1]\n",
    "        out = model(prefix)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "        if logits.dim() == 3:\n",
    "            logits = logits[:, -1, :]  # (B, V)\n",
    "\n",
    "        logp = F.log_softmax(logits, dim=-1)  # (B, V)\n",
    "\n",
    "        # >>> Gather only on valid rows to avoid indexing -100 <<<\n",
    "        y_valid = y_t[valid].long().unsqueeze(1)       # (Bv, 1)\n",
    "        lp_next = logp[valid].gather(1, y_valid).squeeze(1)  # (Bv,)\n",
    "\n",
    "        total_lp[valid] += lp_next\n",
    "        total_cnt[valid] += 1.0\n",
    "\n",
    "    return total_lp / total_cnt.clamp_min(1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e3c8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One mini-batch check\n",
    "(neg_pad, neg_len, comp_start), (pos_pad, pos_len, comp_start2) = next(get_batches_with_compinfo(lines[:64], 8))\n",
    "y_pos = build_completion_targets(pos_pad, pos_len, comp_start2)\n",
    "assert (y_pos >= 0).any(dim=1).all(), \"Found a sample with zero completion tokens.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adaf029",
   "metadata": {},
   "source": [
    "Build batches that also carry completion start indices\n",
    "We need the index where the completion (the part after the shared prompt) starts for each pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15feef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "def get_batches_with_compinfo(lines, batch_size):\n",
    "    \"\"\"\n",
    "    lines: list of (neg_ids, pos_ids) 1D tensors (prompt+completion).\n",
    "    For each pair, compute the shared-prefix length and keep it as the completion start.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        negs, poss = [], []\n",
    "        neg_len, pos_len, comp_start = [], [], []\n",
    "        for (n, p) in batch:\n",
    "            L = common_prefix_len(n, p)\n",
    "            negs.append(n); poss.append(p)\n",
    "            neg_len.append(len(n)); pos_len.append(len(p))\n",
    "            comp_start.append(L)\n",
    "        neg_len = torch.tensor(neg_len, dtype=torch.long)\n",
    "        pos_len = torch.tensor(pos_len, dtype=torch.long)\n",
    "        comp_start = torch.tensor(comp_start, dtype=torch.long)\n",
    "        neg_pad = rnn.pad_sequence(negs, batch_first=True, padding_value=PAD_ID)\n",
    "        pos_pad = rnn.pad_sequence(poss, batch_first=True, padding_value=PAD_ID)\n",
    "        yield (neg_pad, neg_len, comp_start), (pos_pad, pos_len, comp_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bb268326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mean_completion_logprob(model, padded: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Returns mean log-prob per sequence computed ONLY over completion tokens.\n",
    "    'y' must be (B,T) with -100 at positions to ignore.\n",
    "    Works with:\n",
    "      - Fast path: model(padded) -> logits (B,T,V)\n",
    "      - Slow path: model(prefix) -> last-step logits (B,V) or (B,1,V)\n",
    "    \"\"\"\n",
    "    B, T = padded.shape\n",
    "    device = padded.device\n",
    "\n",
    "    # ---------- FAST PATH: full-sequence logits (B,T,V) ----------\n",
    "    out = model(padded)\n",
    "    logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "    if logits.dim() == 3 and logits.size(1) == T:\n",
    "        logp = F.log_softmax(logits, dim=-1)          # (B,T,V)\n",
    "        mask = (y != -100)                            # (B,T) bool\n",
    "\n",
    "        gather_y = y.clone()\n",
    "        gather_y[~mask] = 0                           # safe index\n",
    "        token_lp = logp.gather(2, gather_y.unsqueeze(-1)).squeeze(-1)  # (B,T)\n",
    "\n",
    "        seq_sum = (token_lp * mask.float()).sum(dim=1)                 # (B,)\n",
    "        seq_cnt = mask.float().sum(dim=1).clamp_min(1.0)               # (B,)\n",
    "        return seq_sum / seq_cnt\n",
    "\n",
    "    # ---------- SLOW PATH: last-step logits; loop over time ----------\n",
    "    total_lp = torch.zeros(B, device=device)\n",
    "    total_cnt = torch.zeros(B, device=device)\n",
    "\n",
    "    for t in range(T):\n",
    "        y_t = y[:, t]\n",
    "        valid_idx = (y_t >= 0).nonzero(as_tuple=False).squeeze(1)  # indices where we have a real target\n",
    "        if valid_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        # run ONLY the valid rows for this time-step\n",
    "        prefix = padded[valid_idx, :t+1]                            # (Bv, t+1)\n",
    "        out = model(prefix)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out  # (Bv,V) or (Bv,1,V)\n",
    "        if logits.dim() == 3:\n",
    "            logits = logits[:, -1, :]                               # (Bv,V)\n",
    "\n",
    "        logp = F.log_softmax(logits, dim=-1)                        # (Bv,V)\n",
    "        tgt  = y_t[valid_idx].long().unsqueeze(1)                   # (Bv,1)\n",
    "        lp_next = logp.gather(1, tgt).squeeze(1)                    # (Bv,)\n",
    "\n",
    "        total_lp[valid_idx] += lp_next\n",
    "        total_cnt[valid_idx] += 1.0\n",
    "\n",
    "    return total_lp / total_cnt.clamp_min(1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfea3e",
   "metadata": {},
   "source": [
    "Tiny SFT warm-start on POS completions (1 short epoch)\n",
    "This teaches the pattern “… The answer is 36” explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "35b098e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 50/313 [03:21<15:56,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 50 loss 0.17940084636211395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 100/313 [06:22<12:48,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 100 loss 0.15409192442893982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 150/313 [09:35<12:43,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 150 loss 0.13823696970939636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 200/313 [12:44<06:41,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 200 loss 0.16314968466758728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 250/313 [15:53<04:02,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 250 loss 0.13672390580177307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 299/313 [19:31<00:54,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 300 loss 0.1342238485813141\n",
      "SFT warm-start done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# one quick SFT pass on POS completions only\n",
    "gpt.train()\n",
    "opt_sft = torch.optim.AdamW(gpt.parameters(), lr=3e-4)\n",
    "\n",
    "from tqdm import tqdm\n",
    "BATCH = 16\n",
    "steps = 0\n",
    "for (neg_pad, neg_len, comp_start), (pos_pad, pos_len, comp_start2) in tqdm(get_batches_with_compinfo(lines[:5000], BATCH), total=(min(5000,len(lines))+BATCH-1)//BATCH):\n",
    "    # targets only over completion span\n",
    "    y_pos = build_completion_targets(pos_pad, pos_len, comp_start2).to(device)\n",
    "    pos_pad = pos_pad.to(device)\n",
    "    out = gpt(pos_pad)                      # forward WITHOUT y; we compute CE ourselves\n",
    "    logits = out[0] if isinstance(out,(tuple,list)) else out   # (B,T,V) or (B,1,V)\n",
    "    if logits.dim()==3 and logits.size(1)==pos_pad.size(1):\n",
    "        loss = F.cross_entropy(logits.transpose(1,2), y_pos, ignore_index=-100)\n",
    "    else:\n",
    "        # fallback: loop over time (slower)\n",
    "        loss = 0.0\n",
    "        B,T = pos_pad.shape\n",
    "        for t in range(T):\n",
    "            valid = (y_pos[:, t] >= 0)\n",
    "            if not bool(valid.any()): continue\n",
    "            out = gpt(pos_pad[:, :t+1])\n",
    "            logits = out[0] if isinstance(out,(tuple,list)) else out\n",
    "            if logits.dim() == 3: logits = logits[:, -1, :]\n",
    "            loss = loss + F.cross_entropy(logits[valid], y_pos[valid, t], ignore_index=-100)\n",
    "        loss = loss / T\n",
    "\n",
    "    opt_sft.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_sft.step()\n",
    "    steps += 1\n",
    "    if steps % 50 == 0:\n",
    "        print(\"SFT step\", steps, \"loss\", float(loss))\n",
    "    if steps >= 300:     # ~300 updates is enough for a warm-start\n",
    "        break\n",
    "print(\"SFT warm-start done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545d220",
   "metadata": {},
   "source": [
    "DPO — completion-only objective\n",
    "Now redo (or continue) DPO with the completion-only mean log-prob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb1b3221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [9:19:45<00:00,  5.37s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 mean DPO loss: 0.0000\n",
      "✅ Saved ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "beta = 0.1\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=3e-4)\n",
    "\n",
    "EPOCHS = 1  # start with 1, you can bump to 2 later\n",
    "BATCH = 16\n",
    "total = (len(lines)+BATCH-1)//BATCH\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    gpt.train()\n",
    "    running, nst = 0.0, 0\n",
    "    for (neg_pad, neg_len, comp_start), (pos_pad, pos_len, comp_start2) in tqdm(get_batches_with_compinfo(lines, BATCH), total=total):\n",
    "        neg_pad = neg_pad.to(device); pos_pad = pos_pad.to(device)\n",
    "        neg_len = neg_len.to(device); pos_len = pos_len.to(device)\n",
    "        comp_start = comp_start.to(device)            # same for both\n",
    "\n",
    "        y_neg = build_completion_targets(neg_pad, neg_len, comp_start)\n",
    "        y_pos = build_completion_targets(pos_pad, pos_len, comp_start)\n",
    "\n",
    "        neg_lp = mean_completion_logprob(gpt, neg_pad, y_neg)   # (B,)\n",
    "        pos_lp = mean_completion_logprob(gpt, pos_pad, y_pos)   # (B,)\n",
    "\n",
    "        loss = -F.logsigmoid((pos_lp - neg_lp) / beta).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += float(loss); nst += 1\n",
    "    print(f\"Epoch {epoch+1} mean DPO loss: {running/max(nst,1):.4f}\")\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": gpt.state_dict(),\n",
    "    \"model_args\": checkpoint['model_args'],\n",
    "}, \"./dpo.pt\")\n",
    "print(\"✅ Saved ./dpo.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "84c84299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 50/3125 [03:25<3:11:54,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 50 loss 0.1099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 100/3125 [06:16<2:33:42,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 100 loss 0.1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 150/3125 [09:08<2:57:53,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 150 loss 0.1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 200/3125 [11:57<2:43:27,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 200 loss 0.1265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 250/3125 [14:39<2:23:20,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 250 loss 0.1158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 300/3125 [17:41<3:09:02,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 300 loss 0.1005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 350/3125 [21:01<2:52:30,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 350 loss 0.1378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 400/3125 [23:47<2:19:32,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 400 loss 0.1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 450/3125 [26:31<2:21:25,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 450 loss 0.0947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 500/3125 [29:23<2:23:02,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 500 loss 0.1307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 550/3125 [32:02<2:19:10,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 550 loss 0.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 600/3125 [34:51<2:32:51,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 600 loss 0.1252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 650/3125 [37:32<2:25:21,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 650 loss 0.1109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 700/3125 [40:21<2:24:59,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 700 loss 0.1338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 750/3125 [42:59<2:01:40,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 750 loss 0.1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 800/3125 [45:37<1:55:44,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 800 loss 0.1438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 850/3125 [48:12<2:00:12,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 850 loss 0.1062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 900/3125 [51:06<2:03:09,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 900 loss 0.0949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 950/3125 [53:52<1:57:48,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 950 loss 0.0932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 1000/3125 [56:35<2:00:08,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1000 loss 0.1182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 1050/3125 [59:33<2:18:05,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1050 loss 0.0931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1100/3125 [1:02:49<2:10:09,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1100 loss 0.0922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1150/3125 [1:35:15<2:20:56,  4.28s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1150 loss 0.0815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1200/3125 [1:38:32<2:18:25,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1200 loss 0.1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1250/3125 [1:41:54<2:00:49,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1250 loss 0.1095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 1300/3125 [1:45:11<1:54:22,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1300 loss 0.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1350/3125 [1:48:26<1:57:26,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1350 loss 0.0999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 1400/3125 [1:51:34<1:47:20,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1400 loss 0.1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 1450/3125 [1:54:49<1:46:11,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1450 loss 0.1018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1500/3125 [1:58:08<1:41:23,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1500 loss 0.0773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 1550/3125 [2:01:04<1:21:08,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1550 loss 0.0905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1600/3125 [2:03:49<1:32:49,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1600 loss 0.1033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1650/3125 [2:06:58<1:30:29,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1650 loss 0.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 1700/3125 [2:13:41<8:52:00, 22.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1700 loss 0.0898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1750/3125 [2:16:26<1:09:17,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1750 loss 0.1217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1800/3125 [2:19:05<1:31:30,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1800 loss 0.0780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 1850/3125 [2:21:46<1:10:04,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1850 loss 0.0880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 1900/3125 [2:24:40<1:08:52,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1900 loss 0.0921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1950/3125 [2:27:34<1:26:08,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 1950 loss 0.1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 2000/3125 [2:30:20<1:03:41,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2000 loss 0.0809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 2050/3125 [2:32:59<53:57,  3.01s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2050 loss 0.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2100/3125 [2:35:35<54:00,  3.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2100 loss 0.0816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 2150/3125 [2:38:11<48:51,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2150 loss 0.0852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2200/3125 [2:40:47<45:51,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2200 loss 0.1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 2250/3125 [2:43:29<47:56,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2250 loss 0.1043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 2300/3125 [2:46:23<45:21,  3.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2300 loss 0.1064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 2350/3125 [2:49:01<37:27,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2350 loss 0.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 2400/3125 [2:51:57<47:26,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2400 loss 0.0846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 2450/3125 [2:55:28<56:20,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2450 loss 0.1149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2500/3125 [2:58:34<37:38,  3.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2500 loss 0.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 2550/3125 [3:01:30<38:51,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2550 loss 0.0935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 2600/3125 [3:04:31<31:35,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2600 loss 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 2650/3125 [3:07:10<23:41,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2650 loss 0.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 2700/3125 [3:09:46<22:01,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2700 loss 0.1031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 2750/3125 [3:12:28<23:20,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2750 loss 0.0977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 2800/3125 [3:15:24<20:16,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2800 loss 0.1012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 2850/3125 [3:18:24<16:32,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2850 loss 0.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2900/3125 [3:21:12<12:26,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2900 loss 0.0988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 2950/3125 [3:24:14<12:30,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 2950 loss 0.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 2999/3125 [3:27:45<08:43,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT step 3000 loss 0.0930\n",
      "✅ SFT top-off saved ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# reuse: build_completion_targets, get_batches_with_compinfo, mean_completion_logprob helpers you have\n",
    "\n",
    "gpt.train()\n",
    "opt_sft = torch.optim.AdamW(gpt.parameters(), lr=3e-4)\n",
    "\n",
    "STEPS = 3000             # do ~600–1000 updates total; you did 300 before\n",
    "BATCH = 16\n",
    "done = 0\n",
    "from tqdm import tqdm\n",
    "\n",
    "for (neg_pad, neg_len, comp_start), (pos_pad, pos_len, comp_start2) in tqdm(\n",
    "        get_batches_with_compinfo(lines[:50000], BATCH),\n",
    "        total=(min(50000, len(lines))+BATCH-1)//BATCH):\n",
    "    # targets = completion only for POS\n",
    "    y_pos = build_completion_targets(pos_pad, pos_len, comp_start2).to(device)\n",
    "    pos_pad = pos_pad.to(device)\n",
    "    out = gpt(pos_pad)\n",
    "    logits = out[0] if isinstance(out,(tuple,list)) else out\n",
    "    if logits.dim()==3 and logits.size(1)==pos_pad.size(1):\n",
    "        loss = F.cross_entropy(logits.transpose(1,2), y_pos, ignore_index=-100)\n",
    "    else:\n",
    "        # slow fallback\n",
    "        B,T = pos_pad.shape\n",
    "        loss = 0.0\n",
    "        for t in range(T):\n",
    "            valid = (y_pos[:, t] >= 0)\n",
    "            if not bool(valid.any()): continue\n",
    "            out = gpt(pos_pad[:, :t+1])\n",
    "            logits = out[0] if isinstance(out,(tuple,list)) else out\n",
    "            if logits.dim()==3: logits = logits[:, -1, :]\n",
    "            loss = loss + F.cross_entropy(logits[valid], y_pos[valid, t], ignore_index=-100)\n",
    "        loss = loss / T\n",
    "\n",
    "    opt_sft.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_sft.step()\n",
    "\n",
    "    done += 1\n",
    "    if done % 50 == 0:\n",
    "        print(f\"SFT step {done} loss {float(loss):.4f}\")\n",
    "    if done >= STEPS:\n",
    "        break\n",
    "\n",
    "torch.save({\"model_state_dict\": gpt.state_dict(),\n",
    "            \"model_args\": checkpoint['model_args']},\n",
    "           \"./dpo.pt\")\n",
    "print(\"✅ SFT top-off saved ./dpo.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791cfb0",
   "metadata": {},
   "source": [
    "Evaluation — simple, template-matched, short\n",
    "Keep it minimal and parse only the completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae205e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17+19=?          → out='1 15' | pred=1 | tgt=36 | ✗\n",
      "3*17=?           → out='6 138 becaus' | pred=6 | tgt=51 | ✗\n",
      "72/4=?           → out='4+49' | pred=4 | tgt=18 | ✗\n",
      "72-x=34, x=?     → out='iis 5-542 eq' | pred=None | tgt=38 | ✗\n",
      "x*11=44, x=?     → out='because 1*12' | pred=None | tgt=4 | ✗\n",
      "Accuracy: 0/5\n"
     ]
    }
   ],
   "source": [
    "import re, torch\n",
    "\n",
    "def format_prompt(q: str) -> str:\n",
    "    return f\"{q} The answer is \"\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen_and_parse(model, q, max_new_tokens=12, temperature=0.6, top_k=40):\n",
    "    prompt = format_prompt(q)\n",
    "    x = torch.tensor([stoi.get(ch,0) for ch in prompt], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    y = model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "    if isinstance(y, tuple): y = y[0]\n",
    "    txt = decode(y)\n",
    "    comp = txt[len(prompt):]\n",
    "    # stop at common delimiters\n",
    "    for stop in [\" because\", \"\\n\", \".\", \" Answer\", \"The answer is\"]:\n",
    "        j = comp.find(stop)\n",
    "        if j > 0:\n",
    "            comp = comp[:j]\n",
    "            break\n",
    "    m = re.search(r\"^[-+]?\\d+\", comp.strip())\n",
    "    pred = int(m.group()) if m else None\n",
    "    return comp, pred\n",
    "\n",
    "tests = [(\"17+19=?\", 36), (\"3*17=?\", 51), (\"72/4=?\", 18), (\"72-x=34, x=?\", 38), (\"x*11=44, x=?\", 4)]\n",
    "ok = 0\n",
    "for q,t in tests:\n",
    "    comp, pred = gen_and_parse(gpt, q, max_new_tokens=12, temperature=0.6, top_k=40)\n",
    "    good = (pred is not None) and (pred == int(t))\n",
    "    ok += int(good)\n",
    "    print(f\"{q:<16} → out={repr(comp)} | pred={pred} | tgt={t} | {'✓' if good else '✗'}\")\n",
    "print(f\"Accuracy: {ok}/{len(tests)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b6ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
